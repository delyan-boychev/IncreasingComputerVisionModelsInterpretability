{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz9f5-d8_DFh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx9sMOUlITom"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install torchvision\n",
        "!pip install shap\n",
        "!git clone https://github.com/MadryLab/robustness\n",
        "!pip install ./robustness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-f3kJcyI4al"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import torch\n",
        "from robustness import model_utils, datasets, train, defaults\n",
        "import timm\n",
        "from cox.utils import Parameters\n",
        "import cox.store\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "ds = datasets.CIFAR(data_path='./data')\n",
        "\n",
        "model = timm.create_model('ecaresnet26t', num_classes=10, pretrained=True)\n",
        "m, _ = model_utils.make_and_restore_model(arch=model,resume_path=\"/content/gdrive/My Drive/Final Adversarial CIFAR10/checkpoint.pt.best\", dataset=ds, add_custom_forward=True)\n",
        "train_loader, val_loader = ds.make_loaders(batch_size=20, workers=4)\n",
        "\n",
        "# Create a cox store for logging\n",
        "out_store = cox.store.Store(\"/content/gdrive/My Drive\")\n",
        "\n",
        "# Hard-coded base parameters\n",
        "train_kwargs = {\n",
        "    'out_dir': \"train_out\",\n",
        "    'adv_train': 1,\n",
        "    'constraint': '2',\n",
        "    'eps': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'save_ckpt_iters': 1,\n",
        "    'attack_lr': 1.5, \n",
        "}\n",
        "train_args = Parameters(train_kwargs)\n",
        "\n",
        "# Fill whatever parameters are missing from the defaults\n",
        "train_args = defaults.check_and_fill_args(train_args,\n",
        "                        defaults.TRAINING_ARGS, datasets.CIFAR)\n",
        "train_args = defaults.check_and_fill_args(train_args,\n",
        "                        defaults.PGD_ARGS, datasets.CIFAR)\n",
        "# Train a model\n",
        "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import torch\n",
        "from robustness import model_utils, datasets, train, defaults\n",
        "import timm\n",
        "from cox.utils import Parameters\n",
        "import cox.store\n",
        "from google.colab import drive\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "    [transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10), transforms.ToTensor()])\n",
        "drive.mount('/content/gdrive')\n",
        "transforms_kwargs = {\n",
        "        \"transform_train\":transform_train\n",
        "}\n",
        "ds = datasets.CIFAR(data_path='./data', **transforms_kwargs)\n",
        "\n",
        "model = timm.create_model('ecaresnet26t', num_classes=10, pretrained=True)\n",
        "m, _ = model_utils.make_and_restore_model(arch=model, dataset=ds, add_custom_forward=True)\n",
        "train_loader, val_loader = ds.make_loaders(batch_size=20, workers=4)\n",
        "\n",
        "# Create a cox store for logging\n",
        "out_store = cox.store.Store(\"/content/gdrive/My Drive\")\n",
        "\n",
        "# Hard-coded base parameters\n",
        "train_kwargs = {\n",
        "    'out_dir': \"train_out\",\n",
        "    'adv_train': 1,\n",
        "    'constraint': '2',\n",
        "    'eps': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'save_ckpt_iters': 1,\n",
        "    'attack_lr': 1.5, \n",
        "}\n",
        "train_args = Parameters(train_kwargs)\n",
        "\n",
        "# Fill whatever parameters are missing from the defaults\n",
        "train_args = defaults.check_and_fill_args(train_args,\n",
        "                        defaults.TRAINING_ARGS, datasets.CIFAR)\n",
        "train_args = defaults.check_and_fill_args(train_args,\n",
        "                        defaults.PGD_ARGS, datasets.CIFAR)\n",
        "# Train a model\n",
        "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)"
      ],
      "metadata": {
        "id": "Lf6VKz0Uxp6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.CIFAR(data_path='./data')\n",
        "\n",
        "model = timm.create_model('ecaresnet26t', num_classes=10, pretrained=True)\n",
        "m, _ = model_utils.make_and_restore_model(arch=model,resume_path=\"/content/gdrive/My Drive/AdversarialModel/ecaresnet26t-CIFAR10-Adv.pt\", dataset=ds, add_custom_forward=True)\n",
        "\n",
        "torch_dict = torch.load(\"/content/gdrive/My Drive/AdversarialModel/ecaresnet26t-CIFAR10-Adv.pt\")[\"model\"]\n",
        "oldnames = []\n",
        "keysToDelete = []\n",
        "names = []\n",
        "for key in torch_dict:\n",
        "  if \"module.model.model.\" in key:\n",
        "    oldnames.append(key)\n",
        "    names.append(key.replace(\"module.model.model.\", \"\"))\n",
        "  else:\n",
        "    keysToDelete.append(key)\n",
        "for i in range(len(oldnames)):\n",
        "  torch_dict[names[i]] = torch_dict.pop(oldnames[i])\n",
        "for key in keysToDelete:\n",
        "  torch_dict.pop(key)\n",
        "model.load_state_dict(torch_dict)\n",
        "train_loader, val_loader = ds.make_loaders(batch_size=20, workers=4)\n",
        "\n",
        "class InputNormalize(torch.nn.Module):\n",
        "    '''\n",
        "    A module (custom layer) for normalizing the input to have a fixed \n",
        "    mean and standard deviation (user-specified).\n",
        "    '''\n",
        "    def __init__(self, new_mean, new_std):\n",
        "        super(InputNormalize, self).__init__()\n",
        "        new_std = new_std[..., None, None]\n",
        "        new_mean = new_mean[..., None, None]\n",
        "\n",
        "        self.register_buffer(\"new_mean\", new_mean)\n",
        "        self.register_buffer(\"new_std\", new_std)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.clamp(x, 0, 1)\n",
        "        x_normalized = (x - self.new_mean)/self.new_std\n",
        "        return x_normalized\n",
        "\n"
      ],
      "metadata": {
        "id": "cuDOZH2LDSZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import logging\n",
        "import numpy as np\n",
        "batch = next(iter(val_loader))\n",
        "images,labels = batch\n",
        "logging.getLogger('shap').setLevel(logging.ERROR)\n",
        "\n",
        "background = images[:10].to(\"cuda\")\n",
        "normalizer = InputNormalize(torch.tensor([0.4914, 0.4822, 0.4465]),torch.tensor([0.2023, 0.1994, 0.2010]))\n",
        "test_images = normalizer(images[10:20])\n",
        "e = shap.DeepExplainer(model, background)\n",
        "shap_values = e.shap_values(test_images)\n",
        "shap_numpy = np.abs([np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]).mean(0)\n",
        "test_numpy = (np.swapaxes(np.swapaxes(images[10:20].numpy(), 1, -1), 1, 2)  * 255).astype(np.uint8)\n",
        "shap.plots.image(shap_numpy, test_numpy)"
      ],
      "metadata": {
        "id": "CVq_LfbogNLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import logging\n",
        "import numpy as np\n",
        "batch = next(iter(val_loader))\n",
        "images,labels = batch\n",
        "logging.getLogger('shap').setLevel(logging.ERROR)\n",
        "\n",
        "background = images[:15].to(\"cuda\")\n",
        "normalizer = InputNormalize(torch.tensor([0.4914, 0.4822, 0.4465]),torch.tensor([0.2023, 0.1994, 0.2010]))\n",
        "test_images = images[15:20].to(\"cuda\")\n",
        "labels = labels.to(\"cuda\")\n",
        "kwargs = {\n",
        "    'constraint':'2', \n",
        "    'eps': 0.5,\n",
        "    'step_size': 1,\n",
        "    'iterations': 20,\n",
        "    'do_tqdm': True,\n",
        "}\n",
        "_, im_adv = m(test_images, labels[15:20], make_adv=True, **kwargs)\n",
        "im_adv = im_adv.to(\"cpu\")\n",
        "test_images = normalizer(im_adv)\n",
        "e = shap.DeepExplainer(model, background)\n",
        "shap_values = e.shap_values(test_images)\n",
        "shap_numpy = np.abs([np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]).mean(0)\n",
        "test_numpy = (np.swapaxes(np.swapaxes(im_adv.numpy(), 1, -1), 1, 2)  * 255).astype(np.uint8)\n",
        "shap.plots.image(shap_numpy, test_numpy)"
      ],
      "metadata": {
        "id": "DcjKqeSuEt56"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CIFAR10 Adversarial Model.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}